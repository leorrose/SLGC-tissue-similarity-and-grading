{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856494bd-971b-4d53-92b6-a772f5934df5",
   "metadata": {},
   "source": [
    "# ***DESI Human Glioma Section Spectra Classification***\n",
    "\n",
    "This notebook shows the process of scrion spectra classification of the DESI Human Glioma preprocessed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df2682-bf3f-4242-94ce-e6bf0b6dcf3c",
   "metadata": {},
   "source": [
    "### ***Import packages***\n",
    "\n",
    "Before we begin, let\"s import all the necessary packages for this notebook.\n",
    "First we add the directory which has our python files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2f72dc-fcd1-47d5-8c2d-fea76cfeadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/sise/assafzar-group/assafzar/Leor/NanoBiopsy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84395cb7-3bb2-46f7-9d9a-794a8eecdab4",
   "metadata": {},
   "source": [
    "Next we import all the necessary packages for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b61342-ee57-4299-a9ce-a8993f8727fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import (filters)\n",
    "from tqdm.notebook import tqdm\n",
    "from pyimzml.ImzMLParser import ImzMLParser, getionimage\n",
    "from nnbiopsy.bn_vae import BNVAE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eebf16-66a7-4b26-8a4b-145d09cbe643",
   "metadata": {},
   "source": [
    "### ***Constants definitions***\n",
    "\n",
    "Next, let\"s define some constant variables for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a05c91-3223-4591-92cc-1ae0d83e8604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder that contains the preprocessed dhg dataset\n",
    "DHG_IN_PATH = \"/sise/assafzar-group/assafzar/Leor/DHG/Preprocessed\"\n",
    "# Define file that contains clinical state anotations\n",
    "LABELS_PATH = \"/sise/assafzar-group/assafzar/Leor/DHG/Clinical_state_anotations.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c71392-9354-46f8-93d3-d1298fab3820",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ***Reading MSI clinical state anotations***\n",
    "\n",
    "Next, lets read the clinical state anotations for each MSI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f91fd0-b6c6-4965-ad0c-180fbd1c74bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read clinical state anotations csv\n",
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "\n",
    "# Filtser only sample_type section = \"s\"\n",
    "labels_df = labels_df[labels_df[\"sample_type\"] == \"s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b293bee-e70a-456b-8a75-c703e2ee0f15",
   "metadata": {},
   "source": [
    "### ***Get all tissue spectra from all MSI:***\n",
    "\n",
    "Next, let\"s get all tissue spectra from all MSI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa78851-8993-4315-9e39-0095303a8735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ea036eeb2e435f86509597e0ddcd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MSI Loop:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create list to save spectras info\n",
    "x_coordinates = []\n",
    "y_coordinates = []\n",
    "sample_names = []\n",
    "sample_nums = []\n",
    "labels = []\n",
    "idxs = []\n",
    "\n",
    "# Loop over each MSI\n",
    "for index, msi_row in tqdm(labels_df.iterrows(), total=labels_df.shape[0], desc=\"MSI Loop\"):\n",
    "  # Parse the MSI file \n",
    "  with ImzMLParser(os.path.join(DHG_IN_PATH, f\"{msi_row.file_name}.imzML\")) as reader:\n",
    "    # Get local TIC image of msi in mz region [600, 900]\n",
    "    local_tic_img = getionimage(reader, 750, tol=150)\n",
    "\n",
    "    # Threshold image to seperate tissue spectra from background\n",
    "    smooth = filters.gaussian(local_tic_img, sigma=1.5)\n",
    "    thresh_mean = filters.threshold_mean(smooth)\n",
    "    thresh_img = local_tic_img > thresh_mean\n",
    "\n",
    "    # Get sample number\n",
    "    sample_num = int(msi_row.file_name.replace(\"HG \", \"\").replace(\"_\", \"-\").split(\"-\")[0])\n",
    "\n",
    "    # Get sample label\n",
    "    sample_label = int(msi_row.who_grade > 2)\n",
    "\n",
    "    # Loop over each spectra\n",
    "    for idx, (x,y,z) in tqdm(enumerate(reader.coordinates), total=len(reader.coordinates), desc=\"Spectra Loop\"):\n",
    "      # Check if spectra is tissue\n",
    "      if thresh_img[y - 1, x - 1]:\n",
    "        # Keep x coordinate of spectra\n",
    "        x_coordinates.append(x)\n",
    "        # Keep y coordinate of spectra\n",
    "        y_coordinates.append(y)\n",
    "        # Keep sample name of spectra\n",
    "        sample_names.append(msi_row.file_name)\n",
    "        # Keep sample number of spectra\n",
    "        sample_nums.append(sample_num)\n",
    "        # Keep sample label of spectra\n",
    "        labels.append(sample_label)\n",
    "        # Keep unique id of spectra\n",
    "        idxs.append(idx)\n",
    "\n",
    "# Convert to numpy array\n",
    "x_coordinates = np.array(x_coordinates)\n",
    "y_coordinates = np.array(y_coordinates)\n",
    "sample_names = np.array(sample_names)\n",
    "sample_nums = np.array(sample_nums)\n",
    "labels = np.array(labels)\n",
    "idxs = np.array(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b465398-b9c5-4501-b1c7-0e068a648e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsers = { file_name: ImzMLParser(os.path.join(DHG_IN_PATH, f\"{file_name}.imzML\")) for file_name in labels_df.file_name.unique()}\n",
    "\n",
    "#\n",
    "def spectra_gen(indexes):\n",
    "  for i in indexes:\n",
    "    file_name = sample_names[i]\n",
    "    idx = idxs[i]\n",
    "    _, spectra = parsers[file_name].getspectrum(int(idx))\n",
    "    yield [spectra, spectra]\n",
    "\n",
    "#\n",
    "def get_tf_dataset(ids):\n",
    "  #\n",
    "  dataset = tf.data.Dataset.from_generator(\n",
    "    spectra_gen, output_types=(tf.float32, tf.float32), output_shapes=((None,), (None,)),\n",
    "    args=ids)\n",
    "\n",
    "  #\n",
    "  dataset = dataset.shuffle(\n",
    "    buffer_size=len(z), seed=0, reshuffle_each_iteration=True)\n",
    "  \n",
    "  #\n",
    "  dataset = dataset.map(lambda i: tf.py_function(\n",
    "    func=load_spectra, inp=[i], Tout=[tf.float32, tf.float32]),\n",
    "                        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  #\n",
    "  dataset = dataset.batch(256).map(_fixup_shape)\n",
    "  #\n",
    "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da83a3-78b5-48c9-a29f-8aa4ae06876a",
   "metadata": {},
   "source": [
    "### ***LOOCV section spectra classification:***\n",
    "\n",
    "Next, let\"s apply LOOCV classification using a simple dense NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c3a2175-3091-4045-ab39-5fb746b3f484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba8b96f06544f098089966d0dd45c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 12:14:48.007377: W tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: ValueError: callback pyfunc_80 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/leorro/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 232, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_80 is not found\n",
      "\n",
      "\n",
      "2022-05-02 12:14:48.007544: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Invalid argument: ValueError: callback pyfunc_80 is not found\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/leorro/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 232, in __call__\n",
      "    raise ValueError(\"callback %s is not found\" % token)\n",
      "\n",
      "ValueError: callback pyfunc_80 is not found\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     15/Unknown - 15s 1s/step - loss: nan   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m vae_model\u001b[38;5;241m.\u001b[39mcompile(optimizer, loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Train the VAE model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Load the saved weights into the model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m vae_model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_filepath)\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:66\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:848\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m traceme\u001b[38;5;241m.\u001b[39mTraceMe(\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    843\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    844\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    845\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m    846\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[1;32m    847\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 848\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m   \u001b[38;5;66;03m# Catch OutOfRangeError for Datasets of unknown size.\u001b[39;00m\n\u001b[1;32m    850\u001b[0m   \u001b[38;5;66;03m# This blocks until the batch has finished executing.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m   \u001b[38;5;66;03m# TODO(b/150292341): Allow multiple async steps here.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39minferred_steps:\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:580\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count():\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_counter\u001b[38;5;241m.\u001b[39mcalled_without_tracing()\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:611\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    609\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    610\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    614\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2420\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2419\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1661\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs):\n\u001b[1;32m   1648\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1661\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1740\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1741\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/function.py:593\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    592\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    602\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    606\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tfgpu_jup/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over each sample number\n",
    "for exclude_sample in tqdm(np.unique(sample_nums)[:1]):\n",
    "  # Clear graph\n",
    "  K.clear_session()\n",
    "  gc.collect()\n",
    "  \n",
    "  # Create filter for training data\n",
    "  train_filter = (sample_nums != exclude_sample)\n",
    "  \n",
    "  # Get unique ids of training data\n",
    "  train_ids = ids[train_filter]\n",
    "  \n",
    "  # Split training to train and validation\n",
    "  train_ids, val_ids, _, _ = train_test_split(\n",
    "    train_ids, labels[train_filter], test_size=0.2,\n",
    "    random_state=0, stratify=labels[train_filter])\n",
    "  \n",
    "  # Create generators\n",
    "  training_generator = get_tf_dataset(train_ids)\n",
    "  validation_generator  = get_tf_dataset(val_ids)\n",
    "  test_generator  = get_tf_dataset(ids=ids[~train_filter])\n",
    "  \n",
    "  # Create Callback to save the NN weights for best epoch by validation\n",
    "  checkpoint_filepath = \"/sise/assafzar-group/assafzar/Leor/NanoBiopsy/nnbiopsy/desi_human_glioma\"\n",
    "  model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True\n",
    "  )\n",
    "  \n",
    "  # Create VAE model\n",
    "  vae_model = BNVAE(92000, 512, 10)\n",
    "  \n",
    "  # Compile the VAE model\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "  vae_model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "  # Train the VAE model\n",
    "  history = vae_model.fit(x=training_generator,\n",
    "                          validation_data=validation_generator,\n",
    "                          epochs=10,\n",
    "                          callbacks=[model_checkpoint_callback])\n",
    "  \n",
    "  # Load the saved weights into the model\n",
    "  vae_model.load_weights(checkpoint_filepath)\n",
    "  \n",
    "  # Evalute The NN on test set\n",
    "  test_eval = vae_model.evaluate(x=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8472c-42ec-4f02-bc66-d047abd766ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predictions = {}\n",
    "true_labels = {}\n",
    "\n",
    "# Loop over each sample number\n",
    "for exclude_sample in tqdm(np.unique(sample_nums)[:3]):\n",
    "  # Clear graph\n",
    "  K.clear_session()\n",
    "  gc.collect()\n",
    "  \n",
    "  # Create filter for training data\n",
    "  train_filter = (sample_nums != exclude_sample)\n",
    "  \n",
    "  # Get unique ids of training data\n",
    "  train_ids = ids[train_filter]\n",
    "  \n",
    "  # Split training to train and validation\n",
    "  train_ids, val_ids, y_train, y_val = train_test_split(\n",
    "    train_ids, labels[train_filter], test_size=0.2,\n",
    "    random_state=0, stratify=labels[train_filter])\n",
    "\n",
    "  # Create dense NN for classification\n",
    "  model = keras.Sequential(\n",
    "      [\n",
    "        keras.Input(shape=(intensities.shape[1],)),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "      ]\n",
    "  )\n",
    "  \n",
    "  # Create Callback to save the NN weights for best validation accuracy epoch\n",
    "  checkpoint_filepath = \"/sise/assafzar-group/assafzar/Leor/NanoBiopsy/nnbiopsy/desi_human_glioma\"\n",
    "  model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True\n",
    "  )\n",
    "  \n",
    "  # Compile the NN\n",
    "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "  # Train the NN\n",
    "  history = model.fit(x=intensities[np.isin(ids, train_ids)],\n",
    "                      y=y_train,\n",
    "                      batch_size=512,\n",
    "                      validation_data=(intensities[np.isin(ids, val_ids)], y_val),\n",
    "                      epochs=3,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[model_checkpoint_callback])\n",
    "  \n",
    "  # Load the saved weights into the model\n",
    "  model.load_weights(checkpoint_filepath)\n",
    "  \n",
    "  # Evalute The NN on test set\n",
    "  test_eval = model.evaluate(x=intensities[~train_filter],\n",
    "                             y=labels[~train_filter],\n",
    "                             batch_size=512)\n",
    "  \n",
    "  #\n",
    "  predictions[exclude_sample] = model.predict(x=intensities[~train_filter]).ravel()\n",
    "  \n",
    "  #\n",
    "  true_labels[exclude_sample] = labels[~train_filter]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380eaf0-d4b8-46bf-b894-71e4bb57d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(\n",
    "  np.concatenate(list(true_labels.values())),\n",
    "  np.concatenate(list(predictions.values())),\n",
    "  pos_label=0)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, \"b\", label = f\"AUC = {roc_auc:.2f}\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.plot([0, 1], [0, 1],\"r--\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu_jup",
   "language": "python",
   "name": "tfgpu_jup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
